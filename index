import re
import string
import numpy as np
from collections import defaultdict
import math

class AIPersianTranslator:
    def __init__(self):
        self.translation_dict = {}
        # AI Components
        self.sentence_patterns = {}
        self.word_embeddings = {}
        self.language_model = defaultdict(lambda: defaultdict(int))
        
        self.initialize_comprehensive_dictionary()
        self.initialize_ai_components()
    
    def initialize_ai_components(self):
        """Initialize AI components for better translation"""
        # Sentence patterns for common structures
        self.sentence_patterns = {
            'question': {
                'what': 'Ú†Ù‡ {noun} {verb}',
                'where': 'Ú©Ø¬Ø§ {verb} {noun}',
                'when': 'Ú©ÛŒ {verb} {noun}',
                'why': 'Ú†Ø±Ø§ {verb} {noun}',
                'how': 'Ú†Ú¯ÙˆÙ†Ù‡ {verb} {noun}',
                'can': 'Ø¢ÛŒØ§ {subject} Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ {verb}',
                'will': 'Ø¢ÛŒØ§ {subject} Ø®ÙˆØ§Ù‡Ø¯ {verb}'
            },
            'statement': {
                'subject_verb_object': '{subject} {object} {verb}',
                'subject_be_adjective': '{subject} {adjective} Ø§Ø³Øª',
                'possessive': '{possessed} {possessor} {verb}'
            }
        }
        
        # Simple word embeddings for semantic similarity
        self.word_embeddings = {
            'go': ['move', 'travel', 'walk', 'run', 'come'],
            'big': ['large', 'huge', 'great', 'massive', 'enormous'],
            'small': ['little', 'tiny', 'mini', 'compact', 'petite'],
            'happy': ['joyful', 'glad', 'pleased', 'delighted', 'cheerful'],
            'sad': ['unhappy', 'depressed', 'miserable', 'sorrowful', 'gloomy'],
            'good': ['nice', 'excellent', 'fine', 'wonderful', 'great'],
            'bad': ['poor', 'terrible', 'awful', 'horrible', 'negative']
        }
    
    def ai_sentence_analyzer(self, english_sentence):
        """AI-powered sentence analysis to understand structure"""
        words = self.preprocess_text(english_sentence)
        
        analysis = {
            'sentence_type': 'statement',  # question, command, statement
            'main_verb': None,
            'subject': None,
            'object': None,
            'tense': 'present',
            'modality': None  # can, will, should, etc.
        }
        
        # Detect sentence type
        if any(word in ['what', 'where', 'when', 'why', 'how', 'can', 'will'] for word in words):
            analysis['sentence_type'] = 'question'
        
        # Detect modality
        modal_verbs = ['can', 'could', 'will', 'would', 'shall', 'should', 'may', 'might', 'must']
        for word in words:
            if word in modal_verbs:
                analysis['modality'] = word
        
        # Detect tense (simplified)
        if any(word in ['was', 'were', 'had', 'did'] for word in words):
            analysis['tense'] = 'past'
        elif any(word.endswith('ing') for word in words):
            analysis['tense'] = 'present_continuous'
        
        # Find subject (simplified)
        pronouns = ['i', 'you', 'he', 'she', 'we', 'they', 'it']
        for word in words:
            if word in pronouns:
                analysis['subject'] = word
                break
        
        return analysis
    
    def ai_semantic_similarity(self, word1, word2):
        """Calculate semantic similarity between words using simple embeddings"""
        if word1 == word2:
            return 1.0
        
        # Check if words are in each other's semantic clusters
        if word1 in self.word_embeddings and word2 in self.word_embeddings[word1]:
            return 0.8
        if word2 in self.word_embeddings and word1 in self.word_embeddings[word2]:
            return 0.8
        
        # Simple character-based similarity
        common_chars = len(set(word1) & set(word2))
        max_len = max(len(word1), len(word2))
        
        return common_chars / max_len if max_len > 0 else 0
    
    def ai_find_best_translation(self, english_word, context_words=[]):
        """AI-powered word translation with context awareness"""
        # Direct dictionary lookup first
        if english_word in self.translation_dict:
            return self.translation_dict[english_word]
        
        # Try semantic similarity for unknown words
        best_similarity = 0
        best_translation = english_word
        
        for known_word, translation in self.translation_dict.items():
            similarity = self.ai_semantic_similarity(english_word, known_word)
            if similarity > best_similarity and similarity > 0.6:
                best_similarity = similarity
                best_translation = translation
        
        return best_translation
    
    def ai_enhanced_syntax(self, english_words, persian_words, sentence_analysis):
        """AI-powered syntax rearrangement"""
        if not persian_words:
            return []
        
        # Remove articles
        filtered_persian = []
        filtered_english = []
        
        for eng, per in zip(english_words, persian_words):
            if eng not in ['the', 'a', 'an'] and per:
                filtered_persian.append(per)
                filtered_english.append(eng)
        
        # Apply AI-based syntax rules based on sentence analysis
        if sentence_analysis['sentence_type'] == 'question':
            return self.ai_handle_question(filtered_english, filtered_persian, sentence_analysis)
        else:
            return self.ai_handle_statement(filtered_english, filtered_persian, sentence_analysis)
    
    def ai_handle_question(self, english_words, persian_words, analysis):
        """AI handling for question sentences"""
        result = []
        
        # For "can" questions
        if analysis['modality'] == 'can':
            # Pattern: Ø¢ÛŒØ§ + subject + Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ + verb + object
            result.append('Ø¢ÛŒØ§')
            
            # Add subject
            for i, eng_word in enumerate(english_words):
                if eng_word in ['i', 'you', 'he', 'she', 'we', 'they']:
                    result.append(persian_words[i])
            
            # Add modal verb
            result.append('Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯')
            
            # Add other words (skip modal and subject)
            for i, eng_word in enumerate(english_words):
                if (eng_word not in ['can', 'i', 'you', 'he', 'she', 'we', 'they'] and
                    persian_words[i] not in result):
                    result.append(persian_words[i])
        
        # For WH-questions (what, where, when, why, how)
        elif any(word in ['what', 'where', 'when', 'why', 'how'] for word in english_words):
            wh_word = None
            wh_index = -1
            
            # Find WH-word
            for i, eng_word in enumerate(english_words):
                if eng_word in ['what', 'where', 'when', 'why', 'how']:
                    wh_word = eng_word
                    wh_index = i
                    break
            
            if wh_word:
                # Add WH-word first
                result.append(persian_words[wh_index])
                
                # Add other words
                for i, eng_word in enumerate(english_words):
                    if i != wh_index and persian_words[i] not in result:
                        result.append(persian_words[i])
        
        return result if result else persian_words
    
    def ai_handle_statement(self, english_words, persian_words, analysis):
        """AI handling for statement sentences"""
        result = []
        
        # Basic SOV structure with AI improvements
        subjects = []
        verbs = []
        objects = []
        adjectives = []
        possessives = {}
        
        # Classify words with AI context
        for i, (eng_word, per_word) in enumerate(zip(english_words, persian_words)):
            # Subjects
            if eng_word in ['i', 'you', 'he', 'she', 'we', 'they']:
                subjects.append(per_word)
            
            # Verbs
            elif eng_word in ['am', 'is', 'are', 'was', 'were']:
                verbs.append('Ø§Ø³Øª')
            elif eng_word in ['have', 'has', 'had']:
                verbs.append('Ø¯Ø§Ø±Ø¯')
            elif eng_word in self.translation_dict and any(x in self.translation_dict[eng_word] for x in ['Ù…ÛŒâ€Œ', 'Ú©Ø±Ø¯', 'Ø§Ø³Øª']):
                verbs.append(per_word)
            
            # Possessives
            elif eng_word in ['my', 'your', 'his', 'her', 'our', 'their']:
                if i + 1 < len(english_words):
                    possessives[i + 1] = per_word
            
            else:
                if i in possessives:
                    objects.append(persian_words[i])
                    objects.append(possessives[i])
                else:
                    objects.append(per_word)
        
        # Build sentence with AI-optimized structure
        result.extend(subjects)
        result.extend(objects)
        result.extend(verbs)
        
        return result if result else persian_words

    def initialize_comprehensive_dictionary(self):
        """Initialize with a much larger vocabulary"""
        base_dict = {
            # Pronouns
            'i': 'Ù…Ù†', 'you': 'ØªÙˆ', 'he': 'Ø§Ùˆ', 'she': 'Ø§Ùˆ', 'we': 'Ù…Ø§', 'they': 'Ø¢Ù†Ù‡Ø§', 'it': 'Ø¢Ù†',
            'me': 'Ù…Ù†', 'him': 'Ø§Ùˆ', 'her': 'Ø§Ùˆ', 'us': 'Ù…Ø§', 'them': 'Ø¢Ù†Ù‡Ø§',
            'my': 'Ù…Ù†', 'your': 'ØªÙˆ', 'his': 'Ø§Ùˆ', 'her': 'Ø§Ùˆ', 'our': 'Ù…Ø§', 'their': 'Ø¢Ù†Ù‡Ø§',
            
            # Common verbs
            'am': 'Ù‡Ø³ØªÙ…', 'is': 'Ø§Ø³Øª', 'are': 'Ù‡Ø³ØªÛŒØ¯', 'was': 'Ø¨ÙˆØ¯', 'were': 'Ø¨ÙˆØ¯ÛŒØ¯',
            'have': 'Ø¯Ø§Ø±Ù…', 'has': 'Ø¯Ø§Ø±Ø¯', 'had': 'Ø¯Ø§Ø´Øª',
            'do': 'Ù…ÛŒâ€ŒÚ©Ù†Ù…', 'does': 'Ù…ÛŒâ€ŒÚ©Ù†Ø¯', 'did': 'Ú©Ø±Ø¯',
            'go': 'Ø¨Ø±ÙˆÛŒÙ…', 'went': 'Ø±ÙØªÛŒÙ…', 'goes': 'Ù…ÛŒâ€ŒØ±ÙˆØ¯', 'going': 'Ø±ÙØªÙ†',
            'see': 'Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ù…', 'saw': 'Ø¯ÛŒØ¯Ù…', 'seen': 'Ø¯ÛŒØ¯Ù‡',
            'get': 'Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù…', 'got': 'Ú¯Ø±ÙØªÙ…','eat':'Ø®ÙˆØ±Ø¯Ù†',
            'make': 'Ù…ÛŒâ€ŒØ³Ø§Ø²Ù…', 'made': 'Ø³Ø§Ø®ØªÙ…',
            'take': 'Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù…', 'took': 'Ú¯Ø±ÙØªÙ…',
            'come': 'Ù…ÛŒâ€ŒØ¢ÛŒÙ…', 'came': 'Ø¢Ù…Ø¯Ù…',
            'know': 'Ù…ÛŒâ€ŒØ¯Ø§Ù†Ù…', 'knew': 'Ø¯Ø§Ù†Ø³ØªÙ…',
            'think': 'ÙÚ©Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù…', 'thought': 'ÙÚ©Ø± Ú©Ø±Ø¯Ù…',
            'want': 'Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ù…', 'wanted': 'Ø®ÙˆØ§Ø³ØªÙ…',
            'like': 'Ø¯ÙˆØ³Øª Ø¯Ø§Ø±Ù…', 'liked': 'Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ…',
            'love': 'Ø¯ÙˆØ³Øª Ø¯Ø§Ø±Ù…', 'loved': 'Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ…',
            'work': 'Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù…', 'worked': 'Ú©Ø§Ø± Ú©Ø±Ø¯Ù…',
            'play': 'Ø¨Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù…', 'played': 'Ø¨Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù…',
            'read': 'Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ù…', 'reading': 'Ø®ÙˆØ§Ù†Ø¯Ù†',
            'write': 'Ù…ÛŒâ€ŒÙ†ÙˆÛŒØ³Ù…', 'writing': 'Ù†ÙˆØ´ØªÙ†',
            'speak': 'ØµØ­Ø¨Øª Ù…ÛŒâ€ŒÚ©Ù†Ù…', 'speaking': 'ØµØ­Ø¨Øª Ú©Ø±Ø¯Ù†',
            'understand': 'Ù…ÛŒâ€ŒÙÙ‡Ù…Ù…', 'understanding': 'ÙÙ‡Ù…ÛŒØ¯Ù†',
            'learn': 'ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù…', 'learning': 'ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ',
            'teach': 'Ø¢Ù…ÙˆØ²Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ù…', 'teaching': 'Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù†',
            
            # Modal verbs
            'can': 'Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ…', 'could': 'Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø³ØªÛŒÙ…',
            'will': 'Ø®ÙˆØ§Ù‡ÛŒÙ…', 'would': 'Ù…ÛŒâ€ŒÚ©Ø±Ø¯ÛŒÙ…',
            'shall': 'Ø¨Ø§ÛŒØ¯', 'should': 'Ø¨Ø§ÛŒØ¯',
            'may': 'Ù…Ù…Ú©Ù† Ø§Ø³Øª', 'might': 'Ù…Ù…Ú©Ù† Ø¨ÙˆØ¯',
            'must': 'Ø¨Ø§ÛŒØ¯',
            
            # Common nouns
            'time': 'Ø²Ù…Ø§Ù†', 'person': 'Ø´Ø®Øµ', 'people': 'Ù…Ø±Ø¯Ù…', 'man': 'Ù…Ø±Ø¯', 'woman': 'Ø²Ù†', 
            'child': 'Ú©ÙˆØ¯Ú©', 'children': 'Ú©ÙˆØ¯Ú©Ø§Ù†', 'family': 'Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡', 'friend': 'Ø¯ÙˆØ³Øª',
            'house': 'Ø®Ø§Ù†Ù‡', 'home': 'Ø®Ø§Ù†Ù‡', 'room': 'Ø§ØªØ§Ù‚', 'door': 'Ø¯Ø±', 'window': 'Ù¾Ù†Ø¬Ø±Ù‡',
            'car': 'Ù…Ø§Ø´ÛŒÙ†', 'bus': 'Ø§ØªÙˆØ¨ÙˆØ³', 'train': 'Ù‚Ø·Ø§Ø±', 'bike': 'Ø¯ÙˆÚ†Ø±Ø®Ù‡','girl':'Ø¯Ø®ØªØ±','boy':'Ù¾Ø³Ø±',
            'water': 'Ø¢Ø¨', 'food': 'ØºØ°Ø§', 'bread': 'Ù†Ø§Ù†', 'rice': 'Ø¨Ø±Ù†Ø¬', 'fruit': 'Ù…ÛŒÙˆÙ‡',
            'book': 'Ú©ØªØ§Ø¨', 'pen': 'Ø®ÙˆØ¯Ú©Ø§Ø±', 'paper': 'Ú©Ø§ØºØ°', 'computer': 'Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±',
            'phone': 'ØªÙ„ÙÙ†', 'tv': 'ØªÙ„ÙˆÛŒØ²ÛŒÙˆÙ†', 'music': 'Ù…ÙˆØ³ÛŒÙ‚ÛŒ',
            'school': 'Ù…Ø¯Ø±Ø³Ù‡', 'teacher': 'Ù…Ø¹Ù„Ù…', 'student': 'Ø¯Ø§Ù†Ø´Ø¬Ùˆ', 'study': 'Ù…Ø·Ø§Ù„Ø¹Ù‡',
            'work': 'Ú©Ø§Ø±', 'job': 'Ø´ØºÙ„', 'money': 'Ù¾ÙˆÙ„', 'price': 'Ù‚ÛŒÙ…Øª',
            'city': 'Ø´Ù‡Ø±', 'country': 'Ú©Ø´ÙˆØ±', 'world': 'Ø¬Ù‡Ø§Ù†', 'street': 'Ø®ÛŒØ§Ø¨Ø§Ù†',
            'day': 'Ø±ÙˆØ²', 'night': 'Ø´Ø¨', 'week': 'Ù‡ÙØªÙ‡', 'month': 'Ù…Ø§Ù‡', 'year': 'Ø³Ø§Ù„',
            'sun': 'Ø®ÙˆØ±Ø´ÛŒØ¯', 'moon': 'Ù…Ø§Ù‡', 'star': 'Ø³ØªØ§Ø±Ù‡', 'sky': 'Ø¢Ø³Ù…Ø§Ù†',
            'tree': 'Ø¯Ø±Ø®Øª', 'flower': 'Ú¯Ù„', 'animal': 'Ø­ÛŒÙˆØ§Ù†', 'dog': 'Ø³Ú¯', 'cat': 'Ú¯Ø±Ø¨Ù‡',
            'bird': 'Ù¾Ø±Ù†Ø¯Ù‡', 'fish': 'Ù…Ø§Ù‡ÛŒ', 'horse': 'Ø§Ø³Ø¨', 'cow': 'Ú¯Ø§Ùˆ', 'chicken': 'Ù…Ø±Øº',
            'name': 'Ø§Ø³Ù…', 'word': 'Ú©Ù„Ù…Ù‡', 'number': 'Ø¹Ø¯Ø¯', 'part': 'Ù‚Ø³Ù…Øª',
            'problem': 'Ù…Ø´Ú©Ù„', 'fact': 'ÙˆØ§Ù‚Ø¹ÛŒØª', 'way': 'Ø±Ø§Ù‡', 'thing': 'Ú†ÛŒØ²',
            'life': 'Ø²Ù†Ø¯Ú¯ÛŒ', 'hand': 'Ø¯Ø³Øª', 'eye': 'Ú†Ø´Ù…', 'head': 'Ø³Ø±',
            'place': 'Ù…Ú©Ø§Ù†', 'point': 'Ù†Ù‚Ø·Ù‡', 'government': 'Ø¯ÙˆÙ„Øª', 'company': 'Ø´Ø±Ú©Øª',
            'group': 'Ú¯Ø±ÙˆÙ‡', 'case': 'Ù…ÙˆØ±Ø¯', 'table': 'Ù…ÛŒØ²', 'chair': 'ØµÙ†Ø¯Ù„ÛŒ',
            'bed': 'ØªØ®Øª', 'kitchen': 'Ø¢Ø´Ù¾Ø²Ø®Ø§Ù†Ù‡', 'bathroom': 'Ø¯Ø³ØªØ´ÙˆÛŒÛŒ',
            'garden': 'Ø¨Ø§Øº', 'park': 'Ù¾Ø§Ø±Ú©', 'shop': 'ÙØ±ÙˆØ´Ú¯Ø§Ù‡', 'market': 'Ø¨Ø§Ø²Ø§Ø±',
            'hospital': 'Ø¨ÛŒÙ…Ø§Ø±Ø³ØªØ§Ù†', 'doctor': 'Ø¯Ú©ØªØ±', 'medicine': 'Ø¯Ø§Ø±Ùˆ',
            'dinner': 'Ø´Ø§Ù…', 'lunch': 'Ù†Ø§Ù‡Ø§Ø±', 'breakfast': 'ØµØ¨Ø­Ø§Ù†Ù‡',
            
            # Adjectives
            'good': 'Ø®ÙˆØ¨', 'bad': 'Ø¨Ø¯', 'big': 'Ø¨Ø²Ø±Ú¯', 'small': 'Ú©ÙˆÚ†Ú©', 
            'beautiful': 'Ø²ÛŒØ¨Ø§', 'ugly': 'Ø²Ø´Øª', 'happy': 'Ø®ÙˆØ´Ø­Ø§Ù„', 'sad': 'ØºÙ…Ú¯ÛŒÙ†',
            'angry': 'Ø¹ØµØ¨Ø§Ù†ÛŒ', 'tired': 'Ø®Ø³ØªÙ‡', 'hungry': 'Ú¯Ø±Ø³Ù†Ù‡', 'thirsty': 'ØªØ´Ù†Ù‡',
            'hot': 'Ø¯Ø§Øº', 'cold': 'Ø³Ø±Ø¯', 'warm': 'Ú¯Ø±Ù…', 'cool': 'Ø®Ù†Ú©',
            'new': 'Ø¬Ø¯ÛŒØ¯', 'old': 'Ù‚Ø¯ÛŒÙ…ÛŒ', 'young': 'Ø¬ÙˆØ§Ù†', 'fast': 'Ø³Ø±ÛŒØ¹',
            'slow': 'Ø¢Ù‡Ø³ØªÙ‡', 'easy': 'Ø¢Ø³Ø§Ù†', 'difficult': 'Ø³Ø®Øª', 'important': 'Ù…Ù‡Ù…',
            
            # Prepositions
            'in': 'Ø¯Ø±', 'on': 'Ø±ÙˆÛŒ', 'at': 'Ø¯Ø±', 'to': 'Ø¨Ù‡', 'from': 'Ø§Ø²',
            'with': 'Ø¨Ø§', 'without': 'Ø¨Ø¯ÙˆÙ†', 'for': 'Ø¨Ø±Ø§ÛŒ', 'about': 'Ø¯Ø±Ø¨Ø§Ø±Ù‡',
            'by': 'ØªÙˆØ³Ø·', 'of': 'Ø§Ø²',
            
            # Question words
            'what': 'Ú†Ù‡', 'who': 'Ú†Ù‡ Ú©Ø³ÛŒ', 'where': 'Ú©Ø¬Ø§', 'when': 'Ú©ÛŒ',
            'why': 'Ú†Ø±Ø§', 'how': 'Ú†Ú¯ÙˆÙ†Ù‡', 'which': 'Ú©Ø¯Ø§Ù…',
            
            # Common phrases
            'hello': 'Ø³Ù„Ø§Ù…', 'hi': 'Ø³Ù„Ø§Ù…', 'goodbye': 'Ø®Ø¯Ø§Ø­Ø§ÙØ¸', 'bye': 'Ø®Ø¯Ø§Ø­Ø§ÙØ¸',
            'please': 'Ù„Ø·ÙØ§', 'thank you': 'Ù…ØªØ´Ú©Ø±Ù…', 'thanks': 'Ù…Ù…Ù†ÙˆÙ†', 
            'sorry': 'Ù…ØªØ§Ø³ÙÙ…', 'excuse me': 'Ø¨Ø¨Ø®Ø´ÛŒØ¯',
            'yes': 'Ø¨Ù„Ù‡', 'no': 'Ù†Ù‡', 'ok': 'Ø¨Ø§Ø´Ù‡', 'okay': 'Ø¨Ø§Ø´Ù‡',
        }
        
        self.translation_dict = base_dict
    
    def preprocess_text(self, text):
        """Clean and tokenize text"""
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        words = text.split()
        return words
    
    def handle_contractions(self, word):
        """Expand common English contractions"""
        contractions = {
            "i'm": "i am", "you're": "you are", "he's": "he is", "she's": "she is",
            "it's": "it is", "we're": "we are", "they're": "they are",
            "i'll": "i will", "you'll": "you will", "he'll": "he will", "she'll": "she will",
            "we'll": "we will", "they'll": "they will", "don't": "do not",
            "doesn't": "does not", "didn't": "did not", "can't": "cannot",
            "couldn't": "could not", "won't": "will not", "wouldn't": "would not",
            "shouldn't": "should not", "isn't": "is not", "aren't": "are not",
            "wasn't": "was not", "weren't": "were not", "haven't": "have not",
            "hasn't": "has not", "hadn't": "had not", "what's": "what is"
        }
        return contractions.get(word.lower(), word)
    
    def translate_word(self, word):
        """AI-enhanced word translation"""
        # Handle contractions first
        expanded = self.handle_contractions(word)
        if expanded != word:
            return expanded
        
        clean_word = word.strip(string.punctuation).lower()
        
        # Use AI for better translation
        return self.ai_find_best_translation(clean_word)
    
    def translate_sentence(self, english_sentence):
        """AI-powered sentence translation"""
        if not english_sentence.strip():
            return ""
        
        # AI Sentence Analysis
        sentence_analysis = self.ai_sentence_analyzer(english_sentence)
        
        # Preprocess
        words = self.preprocess_text(english_sentence)
        
        # Translate each word with AI
        translated_words = []
        for word in words:
            translation = self.translate_word(word)
            if ' ' in translation:
                translated_words.extend(translation.split())
            else:
                translated_words.append(translation)
        
        # AI-powered syntax rearrangement
        final_translation = self.ai_enhanced_syntax(words, translated_words, sentence_analysis)
        
        # Join and clean
        persian_sentence = ' '.join(final_translation)
        persian_sentence = re.sub(r'\s+', ' ', persian_sentence).strip()
        
        return persian_sentence

def main():
    translator = AIPersianTranslator()
    
    print("=" * 70)
    print("ðŸ¤– AI-POWERED ENGLISH TO PERSIAN TRANSLATOR")
    print("=" * 70)
    
    # Test AI capabilities
    test_cases = [
        "can we go for dinner",
        "what is your name",
    ]
    
    print("\nðŸ§ª AI TRANSLATION TEST:")
    print("-" * 50)
    
    for i, test in enumerate(test_cases, 1):
        translation = translator.translate_sentence(test)
        analysis = translator.ai_sentence_analyzer(test)
        
        print(f"{i:2d}. EN: {test}")
        print(f"    FA: {translation}")
        print(f"    AI Analysis: {analysis['sentence_type']} sentence")
        if analysis['modality']:
            print(f"    Modality: {analysis['modality']}")
        print()
    
    # Interactive session
    print("\nðŸ’¬ AI-POWERED TRANSLATION:")
    print("-" * 50)
    
    while True:
        try:
            user_input = input("\nðŸ“ Enter English text (or 'quit'): ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ðŸ‘‹ Goodbye! Ø®Ø¯Ø§Ø­Ø§ÙØ¸!")
                break
                
            if user_input:
                translation = translator.translate_sentence(user_input)
                analysis = translator.ai_sentence_analyzer(user_input)
                
                print(f"\nâœ… English: {user_input}")
                print(f"âœ… Persian: {translation}")
                print(f"ðŸ¤– AI Analysis: {analysis}")
                
            else:
                print("âš ï¸  Please enter some text!")
                
        except KeyboardInterrupt:
            print("\n\nðŸ‘‹ Goodbye! Ø®Ø¯Ø§Ø­Ø§ÙØ¸!")
            break
        except Exception as e:
            print(f"âŒ Error: {e}")

if __name__ == "__main__":
    main()
