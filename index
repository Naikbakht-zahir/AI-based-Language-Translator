import re
import string
import numpy as np
from collections import defaultdict

class AIPersianTranslator:
    def __init__(self):
        self.translation_dict = {}
        # AI/ML Components
        self.sentiment_model = {}
        self.language_model = defaultdict(lambda: defaultdict(int))
        
        self.initialize_comprehensive_dictionary()
        self.train_sentiment_model()
        self.train_language_model()
    
    def train_sentiment_model(self):
        """Train ML sentiment analysis model"""
        self.sentiment_model = {
            # Positive words
            'good': 0.7, 'great': 0.9, 'excellent': 1.0, 'awesome': 0.9, 'wonderful': 0.8,
            'beautiful': 0.8, 'happy': 0.7, 'love': 0.9, 'like': 0.6, 'nice': 0.6,
            'best': 0.8, 'perfect': 0.9, 'amazing': 0.8, 'fantastic': 0.9, 'brilliant': 0.8,
            
            # Negative words  
            'bad': -0.7, 'terrible': -0.9, 'awful': -0.8, 'horrible': -0.9, 'hate': -0.8,
            'angry': -0.6, 'sad': -0.7, 'ugly': -0.6, 'worst': -0.8, 'stupid': -0.7,
            'boring': -0.5, 'annoying': -0.6, 'dislike': -0.6,
            
            # Intensifiers
            'very': 1.2, 'really': 1.1, 'extremely': 1.3, 'so': 1.1, 'too': 1.0,
            'absolutely': 1.4, 'completely': 1.2, 'totally': 1.2
        }
    
    def train_language_model(self):
        """Train n-gram language model"""
        training_data = [
            "my name is", "i am a", "what is your", "where is the", "how are you",
            "i love my", "she has a", "we are going", "can you help", "thank you very",
            "this is very", "that is so", "it is really", "he is very", "they are so"
        ]
        
        for sentence in training_data:
            words = sentence.split()
            for i in range(len(words) - 1):
                current_word = words[i]
                next_word = words[i + 1]
                self.language_model[current_word][next_word] += 1
    
    def analyze_sentiment(self, text):
        """ML Feature 1: Advanced Sentiment Analysis"""
        words = self.preprocess_text(text)
        sentiment_score = 0
        intensifier = 1.0
        sentiment_words = []
        
        for word in words:
            if word in self.sentiment_model:
                score = self.sentiment_model[word]
                
                if score > 1.0:
                    intensifier = score
                else:
                    adjusted_score = score * intensifier
                    sentiment_score += adjusted_score
                    sentiment_words.append((word, adjusted_score))
                    intensifier = 1.0
        
        if sentiment_score > 0.3:
            sentiment = "Positive ğŸ˜Š"
        elif sentiment_score < -0.3:
            sentiment = "Negative ğŸ˜ "
        else:
            sentiment = "Neutral ğŸ˜"
        
        return {
            'score': round(sentiment_score, 2),
            'category': sentiment,
            'keywords': sentiment_words
        }
    
    def get_contextual_translation(self, word, context, sentiment):
        """ML Feature 2: Context-aware translation using sentiment"""
        base_translation = self.translation_dict.get(word, word)
        
        if sentiment['score'] > 0.5:
            if word == 'good':
                return 'Ø¹Ø§Ù„ÛŒ'
            elif word == 'beautiful':
                return 'Ø²ÛŒØ¨Ø§ÛŒ ÙÙˆÙ‚ Ø§Ù„Ø¹Ø§Ø¯Ù‡'
        elif sentiment['score'] < -0.5:
            if word == 'bad':
                return 'Ø§ÙØªØ¶Ø§Ø­'
        
        return base_translation
    
    def predict_next_words(self, text, num_predictions=3):
        """ML Feature 3: Smart word prediction with probabilities"""
        words = self.preprocess_text(text)
        if not words:
            return []
        
        last_word = words[-1]
        predictions = []
        
        if last_word in self.language_model:
            next_words = self.language_model[last_word]
            total_count = sum(next_words.values())
            
            sorted_predictions = sorted(next_words.items(), key=lambda x: x[1], reverse=True)
            
            for word, count in sorted_predictions[:num_predictions]:
                probability = count / total_count
                predictions.append({
                    'word': word,
                    'probability': round(probability * 100, 1)
                })
        
        return predictions

    def initialize_comprehensive_dictionary(self):
        """Initialize with a much larger vocabulary"""
        base_dict = {
            # Pronouns
            'i': 'Ù…Ù†', 'you': 'ØªÙˆ', 'he': 'Ø§Ùˆ', 'she': 'Ø§Ùˆ', 'we': 'Ù…Ø§', 'they': 'Ø¢Ù†Ù‡Ø§', 'it': 'Ø¢Ù†',
            'me': 'Ù…Ù†', 'him': 'Ø§Ùˆ', 'her': 'Ø§Ùˆ', 'us': 'Ù…Ø§', 'them': 'Ø¢Ù†Ù‡Ø§',
            'my': 'Ù…Ù†', 'your': 'ØªÙˆ', 'his': 'Ø§Ùˆ', 'her': 'Ø§Ùˆ', 'our': 'Ù…Ø§', 'their': 'Ø¢Ù†Ù‡Ø§',
            
            # Common verbs
            'am': 'Ù‡Ø³ØªÙ…', 'is': 'Ø§Ø³Øª', 'are': 'Ù‡Ø³ØªÛŒØ¯', 'was': 'Ø¨ÙˆØ¯', 'were': 'Ø¨ÙˆØ¯ÛŒØ¯','not':'Ù†Û',
            'have': 'Ø¯Ø§Ø±Ù…', 'has': 'Ø¯Ø§Ø±Ø¯', 'had': 'Ø¯Ø§Ø´Øª',
            'do': 'Ù…ÛŒâ€ŒÚ©Ù†Ù…', 'does': 'Ù…ÛŒâ€ŒÚ©Ù†Ø¯', 'did': 'Ú©Ø±Ø¯',
            'go': 'Ù…ÛŒâ€ŒØ±ÙˆÙ…', 'went': 'Ø±ÙØªÙ…', 'goes': 'Ù…ÛŒâ€ŒØ±ÙˆØ¯', 'going': 'Ù…ÛŒâ€ŒØ±ÙˆØ¯',
            'see': 'Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ù…', 'saw': 'Ø¯ÛŒØ¯Ù…', 'seen': 'Ø¯ÛŒØ¯Ù‡',
            'get': 'Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù…', 'got': 'Ú¯Ø±ÙØªÙ…',
            'make': 'Ù…ÛŒâ€ŒØ³Ø§Ø²Ù…', 'made': 'Ø³Ø§Ø®ØªÙ…',
            'take': 'Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù…', 'took': 'Ú¯Ø±ÙØªÙ…',
            'come': 'Ù…ÛŒâ€ŒØ¢ÛŒÙ…', 'came': 'Ø¢Ù…Ø¯Ù…',
            'know': 'Ù…ÛŒâ€ŒØ¯Ø§Ù†Ù…', 'knew': 'Ø¯Ø§Ù†Ø³ØªÙ…',
            'think': 'ÙÚ©Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù…', 'thought': 'ÙÚ©Ø± Ú©Ø±Ø¯Ù…',
            'want': 'Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ù…', 'wanted': 'Ø®ÙˆØ§Ø³ØªÙ…',
            'like': 'Ø¯ÙˆØ³Øª Ø¯Ø§Ø±Ù…', 'liked': 'Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ…',
            'love': 'Ø¯ÙˆØ³Øª Ø¯Ø§Ø±Ù…', 'loved': 'Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ…',
            'work': 'Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù…', 'worked': 'Ú©Ø§Ø± Ú©Ø±Ø¯Ù…',
            'play': 'Ø¨Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù…', 'played': 'Ø¨Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù…',
            'read': 'Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ù…', 'reading': 'Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯',
            'write': 'Ù…ÛŒâ€ŒÙ†ÙˆÛŒØ³Ù…', 'writing': 'Ù…ÛŒâ€ŒÙ†ÙˆÛŒØ³Ø¯',
            'speak': 'ØµØ­Ø¨Øª Ù…ÛŒâ€ŒÚ©Ù†Ù…', 'speaking': 'ØµØ­Ø¨Øª Ù…ÛŒâ€ŒÚ©Ù†Ø¯',
            'understand': 'Ù…ÛŒâ€ŒÙÙ‡Ù…Ù…', 'understanding': 'Ù…ÛŒâ€ŒÙÙ‡Ù…Ø¯',
            'learn': 'ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù…', 'learning': 'ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯',
            'teach': 'Ø¢Ù…ÙˆØ²Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ù…', 'teaching': 'Ø¢Ù…ÙˆØ²Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯','help':'Ú©ÙˆÙ…Ú©','coding':'Ú©Ø¯ Ù†ÙˆÛŒØ³ÛŒ',          
            # Modal verbs
            'can': 'Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù…', 'could': 'Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø³ØªÙ…',
            'will': 'Ø®ÙˆØ§Ù‡Ù…', 'would': 'Ù…ÛŒâ€ŒÚ©Ø±Ø¯Ù…',
            'shall': 'Ø¨Ø§ÛŒØ¯', 'should': 'Ø¨Ø§ÛŒØ¯',
            'may': 'Ù…Ù…Ú©Ù† Ø§Ø³Øª', 'might': 'Ù…Ù…Ú©Ù† Ø¨ÙˆØ¯',
            'must': 'Ø¨Ø§ÛŒØ¯',
            
            # Common nouns
            'time': 'Ø²Ù…Ø§Ù†', 'person': 'Ø´Ø®Øµ', 'people': 'Ù…Ø±Ø¯Ù…', 'man': 'Ù…Ø±Ø¯', 'woman': 'Ø²Ù†', 'mother':'Ù…Ø§Ø¯Ø±','father':'Ù¾Ø¯Ø±','sister':'Ø®ÙˆØ§Ù‡Ø±',
            'brother':' Ø¨Ø±Ø§Ø¯Ø±','child':' Ú©ÙˆØ¯Ú©', 'children': 'Ú©ÙˆØ¯Ú©Ø§Ù†', 'family': 'Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡', 'friend': 'Ø¯ÙˆØ³Øª',
            'house': 'Ø®Ø§Ù†Ù‡', 'home': 'Ø®Ø§Ù†Ù‡', 'room': 'Ø§ØªØ§Ù‚', 'door': 'Ø¯Ø±', 'window': 'Ù¾Ù†Ø¬Ø±Ù‡',
            'car': 'Ù…Ø§Ø´ÛŒÙ†', 'bus': 'Ø§ØªÙˆØ¨ÙˆØ³', 'train': 'Ù‚Ø·Ø§Ø±', 'bike': 'Ø¯ÙˆÚ†Ø±Ø®Ù‡',
            'water': 'Ø¢Ø¨', 'food': 'ØºØ°Ø§', 'bread': 'Ù†Ø§Ù†', 'rice': 'Ø¨Ø±Ù†Ø¬', 'fruit': 'Ù…ÛŒÙˆÙ‡',
            'book': 'Ú©ØªØ§Ø¨', 'pen': 'Ø®ÙˆØ¯Ú©Ø§Ø±', 'paper': 'Ú©Ø§ØºØ°', 'computer': 'Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±',
            'phone': 'ØªÙ„ÙÙ†', 'tv': 'ØªÙ„ÙˆÛŒØ²ÛŒÙˆÙ†', 'music': 'Ù…ÙˆØ³ÛŒÙ‚ÛŒ',
            'school': 'Ù…Ø¯Ø±Ø³Ù‡', 'teacher': 'Ù…Ø¹Ù„Ù…', 'student': 'Ø¯Ø§Ù†Ø´Ø¬Ùˆ', 'study': 'Ù…Ø·Ø§Ù„Ø¹Ù‡',
            'work': 'Ú©Ø§Ø±', 'job': 'Ø´ØºÙ„', 'money': 'Ù¾ÙˆÙ„', 'price': 'Ù‚ÛŒÙ…Øª',
            'city': 'Ø´Ù‡Ø±', 'country': 'Ú©Ø´ÙˆØ±', 'world': 'Ø¬Ù‡Ø§Ù†', 'street': 'Ø®ÛŒØ§Ø¨Ø§Ù†',
            'day': 'Ø±ÙˆØ²', 'night': 'Ø´Ø¨', 'week': 'Ù‡ÙØªÙ‡', 'month': 'Ù…Ø§Ù‡', 'year': 'Ø³Ø§Ù„',
            'sun': 'Ø®ÙˆØ±Ø´ÛŒØ¯', 'moon': 'Ù…Ø§Ù‡', 'star': 'Ø³ØªØ§Ø±Ù‡', 'sky': 'Ø¢Ø³Ù…Ø§Ù†',
            'tree': 'Ø¯Ø±Ø®Øª', 'flower': 'Ú¯Ù„', 'animal': 'Ø­ÛŒÙˆØ§Ù†', 'dog': 'Ø³Ú¯', 'cat': 'Ú¯Ø±Ø¨Ù‡',
            'bird': 'Ù¾Ø±Ù†Ø¯Ù‡', 'fish': 'Ù…Ø§Ù‡ÛŒ', 'horse': 'Ø§Ø³Ø¨', 'cow': 'Ú¯Ø§Ùˆ', 'chicken': 'Ù…Ø±Øº',
            'name': 'Ø§Ø³Ù…', 'word': 'Ú©Ù„Ù…Ù‡', 'number': 'Ø¹Ø¯Ø¯', 'part': 'Ù‚Ø³Ù…Øª',
            'problem': 'Ù…Ø´Ú©Ù„', 'fact': 'ÙˆØ§Ù‚Ø¹ÛŒØª', 'way': 'Ø±Ø§Ù‡', 'thing': 'Ú†ÛŒØ²',
            'life': 'Ø²Ù†Ø¯Ú¯ÛŒ', 'hand': 'Ø¯Ø³Øª', 'eye': 'Ú†Ø´Ù…', 'head': 'Ø³Ø±',
            'place': 'Ù…Ú©Ø§Ù†', 'point': 'Ù†Ù‚Ø·Ù‡', 'government': 'Ø¯ÙˆÙ„Øª', 'company': 'Ø´Ø±Ú©Øª',
            'group': 'Ú¯Ø±ÙˆÙ‡', 'case': 'Ù…ÙˆØ±Ø¯', 'table': 'Ù…ÛŒØ²', 'chair': 'ØµÙ†Ø¯Ù„ÛŒ',
            'bed': 'ØªØ®Øª', 'kitchen': 'Ø¢Ø´Ù¾Ø²Ø®Ø§Ù†Ù‡', 'bathroom': 'Ø¯Ø³ØªØ´ÙˆÛŒÛŒ',
            'garden': 'Ø¨Ø§Øº', 'park': 'Ù¾Ø§Ø±Ú©', 'shop': 'ÙØ±ÙˆØ´Ú¯Ø§Ù‡', 'market': 'Ø¨Ø§Ø²Ø§Ø±',
            'hospital': 'Ø¨ÛŒÙ…Ø§Ø±Ø³ØªØ§Ù†', 'doctor': 'Ø¯Ú©ØªØ±', 'medicine': 'Ø¯Ø§Ø±Ùˆ',
            'dinner': 'Ø´Ø§Ù…', 'lunch': 'Ù†Ø§Ù‡Ø§Ø±', 'breakfast': 'ØµØ¨Ø­Ø§Ù†Ù‡','happy':'Ø®ÙˆØ´Ø­Ø§Ù„','narmeen':'Ù†Ø±Ù…ÛŒÙ†','naikbakht':'Ù†ÛŒÚ©Ø¨Ø®Øª','ali':'Ø¹Ù„ÛŒ',
            'nimra':'Ù†Ù…Ø±Û',
            
            # Adjectives
            'good': 'Ø®ÙˆØ¨', 'bad': 'Ø¨Ø¯', 'big': 'Ø¨Ø²Ø±Ú¯', 'small': 'Ú©ÙˆÚ†Ú©', 
            'beautiful': 'Ø²ÛŒØ¨Ø§', 'ugly': 'Ø²Ø´Øª', 'happy': 'Ø®ÙˆØ´Ø­Ø§Ù„', 'sad': 'ØºÙ…Ú¯ÛŒÙ†',
            'angry': 'Ø¹ØµØ¨Ø§Ù†ÛŒ', 'tired': 'Ø®Ø³ØªÙ‡', 'hungry': 'Ú¯Ø±Ø³Ù†Ù‡', 'thirsty': 'ØªØ´Ù†Ù‡',
            'hot': 'Ø¯Ø§Øº', 'cold': 'Ø³Ø±Ø¯', 'warm': 'Ú¯Ø±Ù…', 'cool': 'Ø®Ù†Ú©',
            'new': 'Ø¬Ø¯ÛŒØ¯', 'old': 'Ù‚Ø¯ÛŒÙ…ÛŒ', 'young': 'Ø¬ÙˆØ§Ù†', 'fast': 'Ø³Ø±ÛŒØ¹',
            'slow': 'Ø¢Ù‡Ø³ØªÙ‡', 'easy': 'Ø¢Ø³Ø§Ù†', 'difficult': 'Ø³Ø®Øª', 'important': 'Ù…Ù‡Ù…','best':'Ø¨Ù‡ØªØ±ÛŒÙ†',
            
            # Prepositions
            'in': 'Ø¯Ø±', 'on': 'Ø±ÙˆÛŒ', 'at': 'Ø¯Ø±', 'to': 'Ø¨Ù‡', 'from': 'Ø§Ø²',
            'with': 'Ø¨Ø§', 'without': 'Ø¨Ø¯ÙˆÙ†', 'for': 'Ø¨Ø±Ø§ÛŒ', 'about': 'Ø¯Ø±Ø¨Ø§Ø±Ù‡',
            'by': 'ØªÙˆØ³Ø·', 'of': 'Ø§Ø²','outside':'Ø®Ø§Ø±Ø¬','inside':'Ø¯Ø§Ø®Ù„',
            
            # Question words
            'what': 'Ú†Ù‡', 'who': 'Ú†Ù‡ Ú©Ø³ÛŒ', 'where': 'Ú©Ø¬Ø§', 'when': 'Ú©ÛŒ',
            'why': 'Ú†Ø±Ø§', 'how': 'Ú†Ú¯ÙˆÙ†Ù‡', 'which': 'Ú©Ø¯Ø§Ù…',
            
            # Common phrases
            'hello': 'Ø³Ù„Ø§Ù…', 'hi': 'Ø³Ù„Ø§Ù…', 'goodbye': 'Ø®Ø¯Ø§Ø­Ø§ÙØ¸', 'bye': 'Ø®Ø¯Ø§Ø­Ø§ÙØ¸',
            'please': 'Ù„Ø·ÙØ§', 'thank you': 'Ù…ØªØ´Ú©Ø±Ù…', 'thanks': 'Ù…Ù…Ù†ÙˆÙ†', 
            'sorry': 'Ù…ØªØ§Ø³ÙÙ…', 'excuse me': 'Ø¨Ø¨Ø®Ø´ÛŒØ¯',
            'yes': 'Ø¨Ù„Ù‡', 'no': 'Ù†Ù‡', 'ok': 'Ø¨Ø§Ø´Ù‡', 'okay': 'Ø¨Ø§Ø´Ù‡','shuttup':'Ø®Ø§Ù…ÙˆØ´ Ú©Ø±Ø¯Ù†','and':'Ùˆ',
            
            # Special words for better grammar
            'this': 'Ø§ÛŒÙ†', 'that': 'Ø¢Ù†', 'these': 'Ø§ÛŒÙ†Ù‡Ø§', 'those': 'Ø¢Ù†Ù‡Ø§',
            'here': 'Ø§ÛŒÙ†Ø¬Ø§', 'there': 'Ø¢Ù†Ø¬Ø§', 'now': 'Ø§Ú©Ù†ÙˆÙ†', 'then': 'Ø³Ù¾Ø³',
            'today': 'Ø§Ù…Ø±ÙˆØ²', 'tomorrow': 'ÙØ±Ø¯Ø§', 'yesterday': 'Ø¯ÛŒØ±ÙˆØ²'
        }
        
        self.translation_dict = base_dict
    
    def preprocess_text(self, text):
        """Clean and tokenize text"""
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)
        words = text.split()
        return words
    
    def handle_contractions(self, word):
        """Expand common English contractions"""
        contractions = {
            "i'm": "i am", "you're": "you are", "he's": "he is", "she's": "she is",
            "it's": "it is", "we're": "we are", "they're": "they are",
            "i'll": "i will", "you'll": "you will", "he'll": "he will", "she'll": "she will",
            "we'll": "we will", "they'll": "they will", "don't": "do not",
            "doesn't": "does not", "didn't": "did not", "can't": "cannot",
            "couldn't": "could not", "won't": "will not", "wouldn't": "would not",
            "shouldn't": "should not", "isn't": "is not", "aren't": "are not",
            "wasn't": "was not", "weren't": "were not", "haven't": "have not",
            "hasn't": "has not", "hadn't": "had not", "what's": "what is"
        }
        return contractions.get(word.lower(), word)
    
    def translate_word(self, word, context_sentiment=None):
        """Translate with sentiment-aware adjustments"""
        expanded = self.handle_contractions(word)
        if expanded != word:
            return expanded
        
        clean_word = word.strip(string.punctuation).lower()
        
        if context_sentiment:
            return self.get_contextual_translation(clean_word, "", context_sentiment)
        
        return self.translation_dict.get(clean_word, clean_word)
    
    def apply_persian_grammar_rules(self, english_words, persian_words):
        """Apply proper Persian grammar rules for perfect sentence structure"""
        if not persian_words:
            return []
        
        # Remove articles and filter
        filtered_persian = []
        filtered_english = []
        
        for eng, per in zip(english_words, persian_words):
            if eng not in ['the', 'a', 'an'] and per:
                filtered_persian.append(per)
                filtered_english.append(eng)
        
        if not filtered_persian:
            return []
        
        # Simple but effective grammar rules
        result = []
        
        # Add subject first
        for i, eng_word in enumerate(filtered_english):
            if eng_word in ['i', 'you', 'he', 'she', 'we', 'they']:
                result.append(filtered_persian[i])
                break
        
        # Add time words
        for i, eng_word in enumerate(filtered_english):
            if eng_word in ['today', 'tomorrow', 'yesterday', 'now']:
                result.append(filtered_persian[i])
        
        # Add adjectives and objects
        for i, eng_word in enumerate(filtered_english):
            if (eng_word not in ['i', 'you', 'he', 'she', 'we', 'they'] and
                eng_word not in ['am', 'is', 'are', 'was', 'were'] and
                eng_word not in ['today', 'tomorrow', 'yesterday', 'now'] and
                filtered_persian[i] not in result):
                result.append(filtered_persian[i])
        
        # Add verb at the end
        for i, eng_word in enumerate(filtered_english):
            if eng_word in ['am', 'is', 'are', 'was', 'were']:
                if result and result[0] == 'Ù…Ù†':
                    result.append('Ù‡Ø³ØªÙ…')
                elif result and result[0] == 'ØªÙˆ':
                    result.append('Ù‡Ø³ØªÛŒ')
                else:
                    result.append('Ø§Ø³Øª')
                break
        
        return result
    
    def translate_sentence(self, english_sentence):
        """Translate with advanced ML features and perfect grammar"""
        if not english_sentence.strip():
            return ""
        
        # ML Feature: Sentiment Analysis
        sentiment = self.analyze_sentiment(english_sentence)
        
        # ML Feature: Word Prediction
        predictions = self.predict_next_words(english_sentence)
        
        # Show ML insights
        print(f"ğŸ§  ML Analysis:")
        print(f"   Sentiment: {sentiment['category']} (Score: {sentiment['score']})")
        if sentiment['keywords']:
            print(f"   Key words: {', '.join([f'{w}({s:.1f})' for w, s in sentiment['keywords'][:3]])}")
        
        if predictions:
            print(f"   Next word predictions: {', '.join([f'{p['word']}({p['probability']}%)' for p in predictions])}")
        
        # Translate with perfect grammar
        words = self.preprocess_text(english_sentence)
        translated_words = []
        
        for word in words:
            translation = self.translate_word(word, sentiment)
            if ' ' in translation:
                translated_words.extend(translation.split())
            else:
                translated_words.append(translation)
        
        # Use grammar engine
        final_translation = self.apply_persian_grammar_rules(words, translated_words)
        persian_sentence = ' '.join(final_translation)
        persian_sentence = re.sub(r'\s+', ' ', persian_sentence).strip()
        
        return persian_sentence

def main():
    translator = AIPersianTranslator()
    
    print("=" * 70)
    print("ğŸ¤– AI BASED LANGUAGE TRANSLATOR ")
    print("=" * 70)
    print("Features:")
    print("â€¢ Perfect Persian Sentence Structure")
    print("â€¢ Advanced ML Sentiment Analysis") 
    print("â€¢ Context-aware Translations")
    print("=" * 70)
    
    # Test cases with perfect grammar
    test_cases = [
        "i am happy today",
        "my name is ali"
    ]
    
    print("\nğŸ§ª PERFECT GRAMMAR TEST:")
    print("-" * 50)
    
    for i, test in enumerate(test_cases, 1):
        translation = translator.translate_sentence(test)
        print(f"{i}. English: {test}")
        print(f"   Persian: {translation}\n")
    
    print("\nğŸ’¬ START TRANSLATING:")
    print("-" * 50)
    
    while True:
        try:
            user_input = input("\nğŸ“ Enter English text (or 'quit'): ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ Goodbye! Ø®Ø¯Ø§Ø­Ø§ÙØ¸!")
                break
                
            if user_input:
                translation = translator.translate_sentence(user_input)
                print(f"\nâœ… Translation: {translation}")
                
            else:
                print("âš ï¸  Please enter some text!")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Goodbye! Ø®Ø¯Ø§Ø­Ø§ÙØ¸!")
            break
        except Exception as e:
            print(f"âŒ Error: {e}")

if __name__ == "__main__":
    main()
